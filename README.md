ğŸ§  Face + Hand Landmark AI â€” Real Time MediaPipe + OpenCV Project

This project is an advanced real time Computer Vision system built using the latest MediaPipe Tasks API and OpenCV.
It detects Face Mesh + Hand Landmarks, recognizes gestures, estimates emotion, supports cursor control, and works perfectly on Python 3.12.

This project goes beyond basic tracking and demonstrates interactive AI + Human Input Systems, making it highly valuable for learning, portfolio, and recruiters.

ğŸš€ Features
ğŸ§‘â€ğŸ¤â€ğŸ§‘ Face AI
âœ” Face Mesh Landmark Detection
âœ” Real-time tracking
âœ” Emotion Estimation
(Happy / Surprised / Neutral)

âœ‹ Hand AI
âœ” Hand Landmark Detection
âœ” Gesture Recognition
Fist
Open Palm
Thumbs Up
âœ” Cursor Control (Move mouse using hand)

ğŸ–¥ System Interaction
âœ” Cursor Control via Hand (PyAutoGUI)
âœ” Volume Control (auto disabled if unsupported â€” no crash)

âš™ Technical
âœ” Works on Python 3.12
âœ” Uses MediaPipe Tasks API (replaces removed mp.solutions)
âœ” Lightweight and Fast
âœ” Real-time webcam input
âœ” Works even if some features fail gracefully

ğŸ“¸ Demo Expectations

When you run the app:

ğŸŸ¢ You should see:
Face mesh dots on your face
Yellow dots on your hand
Emotion text on top-left
Gesture label on screen

ğŸŸ¡ Optional:

Cursor starts moving with your hand if enabled
Volume control may work depending on OS + Python

â–¶ï¸ How To Run The Project
1ï¸âƒ£ Install Dependencies
pip install mediapipe==0.10.31 opencv-python numpy pyautogui pycaw comtypes

2ï¸âƒ£ Download Required Models

Create folder:

models/
Download and place these inside:

Face Model
https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task
Save as:
models/face_landmarker.task

Hand Model
https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task
Save as:
models/hand_landmarker.task

3ï¸âƒ£ Run
python advanced_landmarks_app.py

ğŸ® Controls During Demo
Key	Action
F	Toggle Face Mesh
G	Toggle Hand Tracking
J	Toggle Emotion Detection
E	Toggle Gesture Labels
C	Enable Cursor Control
V	Enable Volume Control (if supported)
H	Show help in console
Q	Quit

ğŸ§ª What to Show in Demo
1ï¸âƒ£ Face Demo
Look at camera
âœ” Face mesh appears
âœ” Emotion text updates when smiling or opening mouth

2ï¸âƒ£ Hand Demo
Show your right or left hand
âœ” Yellow landmark dots appear
âœ” Gesture text appears
Try:
Closed fist â†’ Fist
Open hand â†’ Open Palm
Thumbs up â†’ Thumbs Up

3ï¸âƒ£ Cursor Demo

Press:
C
Move your index finger slowly
âœ” Mouse will follow hand

4ï¸âƒ£ Volume Demo (If Supported)

Press:
V
Pinch thumb + index
Volume changes

If not supported, terminal prints:

Volume Control NOT available on this setup
No crash ğŸ‘

â—ï¸ Notes

MediaPipe removed mp.solutions in new versions
This project uses MediaPipe Tasks API
Works with Python 3.12+
If PyCAW fails, volume automatically disables without crashing
Works on Windows, macOS, Linux (cursor only where supported)

ğŸ›  Tech Stack
Python 3.12
MediaPipe 0.10.31
OpenCV
NumPy
PyAutoGUI
PyCAW (optional)

â­ Why This Project Is Valuable

This is not just a basic demo. It demonstrates:

Realtime AI Processing
Human Gesture Interaction
Modern MediaPipe Tasks API
System Interaction via AI
Practical CV + AI Integration


ğŸ“¬ Contribution & Support

If you like this project:
â­ Star the repo
ğŸ–Š Improve features
ğŸ› Report issues

Happy Building ğŸ‘¨â€ğŸ’»ğŸš€
